covariates <- c("(Intercept)",attributes)
row.names(coefs) <- c("edges", attributes)
names(coefs) <- c("Lasso","Ridge","AIC","BIC","CV","MPLE.Full")
for(i in 1:length(covariates))
{
coefs[i,1] <- coef(lassofit)[i]
coefs[i,2] <- coef(ridgefit)[i]
coefs[i,3] <- coef(bestAIC$BestModel)[i]
coefs[i,4] <- coef(bestBIC$BestModel)[i]
coefs[i,5] <- coef(bestCV$BestModel)[i]
coefs[i,6] <- coef(MPLE.fit)[i]
}
coefs[is.na(coefs)] <- 0
#Create Data Frame of lambdas and AIC/BIcs
criteria <- data.frame(lambda.lasso = best_lambda_lasso,
lambda.ridge = best_lambda_ridge,
AIC = min(bestAIC$BestModels$Criterion),
BIC = min(bestBIC$BestModels$Criterion),
CVMCR = min(bestCV$Subsets$CV))
#Create output list
output <- list(coef = coefs, criteria = criteria, lasso.fit = lassofit,
ridge.fit = ridgefit, AIC.fit = bestAIC,
BIC.fit = bestBIC, subsetCV.fit = bestCV, MPLE.Full.fit = MPLE.fit)
#Return output
return(output)
}
es <- ergm_select(adj_mat, attributes)
es$coef
?"bestglm"
temp <- coefs
temp[which(temp > 0)] = 1
temp
which(temp > 0)
temp <- as.matrix(coefs)
temp[which(temp > 0)] = 1
num.covariates = colSums(temp)
num.covariates
temp
temp[which(temp != 0)] = 1
temp
num.covariates = colSums(temp) - 1
num.covariates = colSums(temp)
num.covariates
ergm_select<-function(adjmat, attributes)
{
#Form Attributes into Formula
tempform<-paste(as.vector(attributes),collapse="+")
formula<-as.formula(paste("adjmat~", tempform, sep=""))
#Create Differece matrix
cat("Calculating Difference Matrix \n")
xy<- ergmMPLE(formula,output="matrix")
#MPLE full model fit
cat("Running MPLE full model fit \n")
tempform2 <- paste(c("edges", as.vector(attributes)), collapse = "+")
formula2<-as.formula(paste("adjmat~", tempform2, sep=""))
MPLE.fit <- ergmMPLE(formula2, output = "fit")
#Lasso
cat("Running Lasso fit \n")
cv.lassofit <- cv.glmnet(x=xy$predictor,y=as.factor(xy$response),
weights=xy$weights,family="binomial")
best_lambda_lasso <- cv.lassofit$lambda.min
lassofit <- glmnet(x=xy$predictor,y=as.factor(xy$response),
weights=xy$weights,family="binomial", lambda=best_lambda_lasso)
#Ridge
cat("Running Ridge fit \n")
cv.ridgefit<-cv.glmnet(x=xy$predictor,y=as.factor(xy$response),weights=xy$weights,family="binomial"
,alpha=0)
best_lambda_ridge<- cv.ridgefit$lambda.min
ridgefit <- glmnet(x=xy$predictor,y=as.factor(xy$response),weights=xy$weights,
family="binomial",alpha=0, lambda=best_lambda_ridge)
#Best Subset
#AIC
cat("Running Best Subset AIC fit \n")
bestAIC <- bestglm(Xy=as.data.frame(cbind(xy$predictor,xy$response)),
IC="AIC",weights=xy$weights, family=binomial)
#BIC
cat("Running Best Subset BIC fit \n")
bestBIC <- bestglm(Xy=as.data.frame(cbind(xy$predictor,xy$response)),
IC="BIC",weights=xy$weights, family=binomial)
#Skip this for now. It seems to only be giving 0 covariate estimates
#Cross-Validation
#cat("Running Best CV fit \n")
#bestCV <- bestglm(Xy=as.data.frame(cbind(xy$predictor,xy$response)),
#                 IC="CV",weights=xy$weights, family=binomial)
#Create dataframe of coefficient of various methods
coefs <- as.data.frame(matrix(0,ncol=5,nrow = length(attributes)+1))
covariates <- c("(Intercept)",attributes)
row.names(coefs) <- c("edges", attributes)
#names(coefs) <- c("Lasso","Ridge","AIC","BIC","CV","MPLE.Full")
names(coefs) <- c("Lasso","Ridge","AIC","BIC","MPLE.Full")
for(i in 1:length(covariates))
{
coefs[i,1] <- coef(lassofit)[i]
coefs[i,2] <- coef(ridgefit)[i]
coefs[i,3] <- coef(bestAIC$BestModel)[i]
coefs[i,4] <- coef(bestBIC$BestModel)[i]
#coefs[i,5] <- coef(bestCV$BestModel)[i]
coefs[i,5] <- coef(MPLE.fit)[i]
}
coefs[is.na(coefs)] <- 0
temp <- as.matrix(coefs)
temp[which(temp != 0)] = 1
num.covariates = colSums(temp)
#Create Data Frame of lambdas and AIC/BIcs
criteria <- data.frame(lambda.lasso = best_lambda_lasso,
lambda.ridge = best_lambda_ridge,
AIC = min(bestAIC$BestModels$Criterion),
BIC = min(bestBIC$BestModels$Criterion),
CVMCR = min(bestCV$Subsets$CV))
#Create output list
output <- list(coef = coefs, model.size = num.covariates, criteria = criteria, lasso.fit = lassofit,
ridge.fit = ridgefit, AIC.fit = bestAIC,
BIC.fit = bestBIC, subsetCV.fit = bestCV, MPLE.Full.fit = MPLE.fit)
#Return output
return(output)
}
adj_mat <- network(as_adjacency_matrix(graph = network[[1]], sparse=FALSE), directed=TRUE)
verts <- dim(as_adjacency_matrix(graph=network[[1]],sparse=FALSE))[1]
#For now, we hand pick the attributes. Later, we will automate this.
##This one takes a long time!
#attributes <- c("asymmetric", "ctriple", "cycle(c(2,3,4,5))", "cyclicalties", "density", "dsp(c(1,2,3,4,5))",
#              "esp(c(1,2,3,4,5))", "gwesp", "gwnsp", "idegree(c(1,2,3,4,5))", "idegreepopularity", "intransitive", "istar(c(1,2,3,4,5))",
#             "isolates", "m2star", "mutual", "nsp(c(1,2,3,4,5))", "odegree(c(1,2,3,4,5))", "ostar(c(1,2,3,4,5))", "receiver", "sender", "ttriple")
##Let's first try a simple one
attributes <- c("cycle(2)", "idegree(2)", "idegree(3)")
es <- ergm_select(adj_mat, attributes)
es
ergm_select<-function(adjmat, attributes, VERBOSE == FALSE)
{
#Form Attributes into Formula
tempform<-paste(as.vector(attributes),collapse="+")
formula<-as.formula(paste("adjmat~", tempform, sep=""))
#Create Differece matrix
cat("Calculating Difference Matrix \n")
xy<- ergmMPLE(formula,output="matrix")
#MPLE full model fit
cat("Running MPLE full model fit \n")
tempform2 <- paste(c("edges", as.vector(attributes)), collapse = "+")
formula2<-as.formula(paste("adjmat~", tempform2, sep=""))
MPLE.fit <- ergmMPLE(formula2, output = "fit")
#Lasso
cat("Running Lasso fit \n")
cv.lassofit <- cv.glmnet(x=xy$predictor,y=as.factor(xy$response),
weights=xy$weights,family="binomial")
best_lambda_lasso <- cv.lassofit$lambda.min
lassofit <- glmnet(x=xy$predictor,y=as.factor(xy$response),
weights=xy$weights,family="binomial", lambda=best_lambda_lasso)
#Ridge
cat("Running Ridge fit \n")
cv.ridgefit<-cv.glmnet(x=xy$predictor,y=as.factor(xy$response),weights=xy$weights,family="binomial"
,alpha=0)
best_lambda_ridge<- cv.ridgefit$lambda.min
ridgefit <- glmnet(x=xy$predictor,y=as.factor(xy$response),weights=xy$weights,
family="binomial",alpha=0, lambda=best_lambda_ridge)
#Best Subset
#AIC
cat("Running Best Subset AIC fit \n")
bestAIC <- bestglm(Xy=as.data.frame(cbind(xy$predictor,xy$response)),
IC="AIC",weights=xy$weights, family=binomial)
#BIC
cat("Running Best Subset BIC fit \n")
bestBIC <- bestglm(Xy=as.data.frame(cbind(xy$predictor,xy$response)),
IC="BIC",weights=xy$weights, family=binomial)
#Skip this for now. It seems to only be giving 0 covariate estimates
#Cross-Validation
#cat("Running Best CV fit \n")
#bestCV <- bestglm(Xy=as.data.frame(cbind(xy$predictor,xy$response)),
#                 IC="CV",weights=xy$weights, family=binomial)
#Create dataframe of coefficient of various methods
coefs <- as.data.frame(matrix(0,ncol=5,nrow = length(attributes)+1))
covariates <- c("(Intercept)",attributes)
row.names(coefs) <- c("edges", attributes)
#names(coefs) <- c("Lasso","Ridge","AIC","BIC","CV","MPLE.Full")
names(coefs) <- c("Lasso","Ridge","AIC","BIC","MPLE.Full")
for(i in 1:length(covariates))
{
coefs[i,1] <- coef(lassofit)[i]
coefs[i,2] <- coef(ridgefit)[i]
coefs[i,3] <- coef(bestAIC$BestModel)[i]
coefs[i,4] <- coef(bestBIC$BestModel)[i]
#coefs[i,5] <- coef(bestCV$BestModel)[i]
coefs[i,5] <- coef(MPLE.fit)[i]
}
coefs[is.na(coefs)] <- 0
temp <- as.matrix(coefs)
temp[which(temp != 0)] = 1
num.covariates = colSums(temp)
#Create Data Frame of lambdas and AIC/BIcs
criteria <- data.frame(lambda.lasso = best_lambda_lasso,
lambda.ridge = best_lambda_ridge,
AIC = min(bestAIC$BestModels$Criterion),
BIC = min(bestBIC$BestModels$Criterion),
CVMCR = min(bestCV$Subsets$CV))
#Create output list
if(VERBOSE == TRUE){
output <- list(coef = coefs, model.size = num.covariates, criteria = criteria, lasso.fit = lassofit,
ridge.fit = ridgefit, AIC.fit = bestAIC,
BIC.fit = bestBIC, subsetCV.fit = bestCV, MPLE.Full.fit = MPLE.fit)
}
if(VERBOSE == FALSE){
output <- list(coef = coefs, model.size = num.covariates)
}
#Return output
return(output)
}
ergm_select<-function(adjmat, attributes, VERBOSE = FALSE)
{
#Form Attributes into Formula
tempform<-paste(as.vector(attributes),collapse="+")
formula<-as.formula(paste("adjmat~", tempform, sep=""))
#Create Differece matrix
cat("Calculating Difference Matrix \n")
xy<- ergmMPLE(formula,output="matrix")
#MPLE full model fit
cat("Running MPLE full model fit \n")
tempform2 <- paste(c("edges", as.vector(attributes)), collapse = "+")
formula2<-as.formula(paste("adjmat~", tempform2, sep=""))
MPLE.fit <- ergmMPLE(formula2, output = "fit")
#Lasso
cat("Running Lasso fit \n")
cv.lassofit <- cv.glmnet(x=xy$predictor,y=as.factor(xy$response),
weights=xy$weights,family="binomial")
best_lambda_lasso <- cv.lassofit$lambda.min
lassofit <- glmnet(x=xy$predictor,y=as.factor(xy$response),
weights=xy$weights,family="binomial", lambda=best_lambda_lasso)
#Ridge
cat("Running Ridge fit \n")
cv.ridgefit<-cv.glmnet(x=xy$predictor,y=as.factor(xy$response),weights=xy$weights,family="binomial"
,alpha=0)
best_lambda_ridge<- cv.ridgefit$lambda.min
ridgefit <- glmnet(x=xy$predictor,y=as.factor(xy$response),weights=xy$weights,
family="binomial",alpha=0, lambda=best_lambda_ridge)
#Best Subset
#AIC
cat("Running Best Subset AIC fit \n")
bestAIC <- bestglm(Xy=as.data.frame(cbind(xy$predictor,xy$response)),
IC="AIC",weights=xy$weights, family=binomial)
#BIC
cat("Running Best Subset BIC fit \n")
bestBIC <- bestglm(Xy=as.data.frame(cbind(xy$predictor,xy$response)),
IC="BIC",weights=xy$weights, family=binomial)
#Skip this for now. It seems to only be giving 0 covariate estimates
#Cross-Validation
#cat("Running Best CV fit \n")
#bestCV <- bestglm(Xy=as.data.frame(cbind(xy$predictor,xy$response)),
#                 IC="CV",weights=xy$weights, family=binomial)
#Create dataframe of coefficient of various methods
coefs <- as.data.frame(matrix(0,ncol=5,nrow = length(attributes)+1))
covariates <- c("(Intercept)",attributes)
row.names(coefs) <- c("edges", attributes)
#names(coefs) <- c("Lasso","Ridge","AIC","BIC","CV","MPLE.Full")
names(coefs) <- c("Lasso","Ridge","AIC","BIC","MPLE.Full")
for(i in 1:length(covariates))
{
coefs[i,1] <- coef(lassofit)[i]
coefs[i,2] <- coef(ridgefit)[i]
coefs[i,3] <- coef(bestAIC$BestModel)[i]
coefs[i,4] <- coef(bestBIC$BestModel)[i]
#coefs[i,5] <- coef(bestCV$BestModel)[i]
coefs[i,5] <- coef(MPLE.fit)[i]
}
coefs[is.na(coefs)] <- 0
temp <- as.matrix(coefs)
temp[which(temp != 0)] = 1
num.covariates = colSums(temp)
#Create Data Frame of lambdas and AIC/BIcs
criteria <- data.frame(lambda.lasso = best_lambda_lasso,
lambda.ridge = best_lambda_ridge,
AIC = min(bestAIC$BestModels$Criterion),
BIC = min(bestBIC$BestModels$Criterion),
CVMCR = min(bestCV$Subsets$CV))
#Create output list
if(VERBOSE == TRUE){
output <- list(coef = coefs, model.size = num.covariates, criteria = criteria, lasso.fit = lassofit,
ridge.fit = ridgefit, AIC.fit = bestAIC,
BIC.fit = bestBIC, subsetCV.fit = bestCV, MPLE.Full.fit = MPLE.fit)
}
if(VERBOSE == FALSE){
output <- list(coef = coefs, model.size = num.covariates)
}
#Return output
return(output)
}
es <- ergm_select(adj_mat, attributes, VERBOSE = FALSE)
es
es <- ergm_select(adj_mat, attributes, VERBOSE = FALSE)
es
es <- ergm_select(adj_mat, attributes, VERBOSE = FALSE)
attributes
es
26*26
26*25
26^3*10^4
10^4
10000*(26^3)
26*25*24*10*9*8*7
8!
factorial(8)
factorial(8)/16
install.packages("combinat")
require(combinat)
combn(8,2) + combn(6,2) + combn(4,2) + 1
combn(8,2)
length(combn(8,2)) + length(combn(6,2)) + length(combn(4,2)) + 1
length(combn(8,2)) * length(combn(6,2)) * length(combn(4,2))
35/12
diceN <- function(n) {
randDice = matrix(sample.int(6, size=n*n, replace=TRUE), nrow=n, ncol=n, byrow=TRUE)
means = apply(randDice, 1, mean)
hist(means, breaks=20, col="red", xlab="Sample Means")
#cat("Sample mean: ", mean(means), "\n")
#cat("Sample standard deviation: ", sd(means), "\n")
#cat("sigma_xbar: ", sd(c(1,2,3,4,5,6))/sqrt(n), "\n")
#cat("Percentage error:", 100*(sd(means)-sd(c(1,2,3,4,5,6))/sqrt(n))/(sd(c(1,2,3,4,5,6))/sqrt(n)), "\n")
cat("Population sd 1:", sd(c(1,2,3,4,5,6))/sqrt(n))
cat("Population sd 2", sqrt(2.917 / n))
}
diceN(100)
diceN <- function(n) {
randDice = matrix(sample.int(6, size=n*n, replace=TRUE), nrow=n, ncol=n, byrow=TRUE)
means = apply(randDice, 1, mean)
hist(means, breaks=20, col="red", xlab="Sample Means")
#cat("Sample mean: ", mean(means), "\n")
#cat("Sample standard deviation: ", sd(means), "\n")
#cat("sigma_xbar: ", sd(c(1,2,3,4,5,6))/sqrt(n), "\n")
#cat("Percentage error:", 100*(sd(means)-sd(c(1,2,3,4,5,6))/sqrt(n))/(sd(c(1,2,3,4,5,6))/sqrt(n)), "\n")
cat("Population sd 1:", sd(c(1,2,3,4,5,6))/sqrt(n), "\n")
cat("Population sd 2", sqrt(2.917 / n))
}
diceN(100)
diceN(1000)
diceN <- function(n) {
randDice = matrix(sample.int(6, size=n*n, replace=TRUE), nrow=n, ncol=n, byrow=TRUE)
means = apply(randDice, 1, mean)
hist(means, breaks=20, col="red", xlab="Sample Means")
#cat("Sample mean: ", mean(means), "\n")
#cat("Sample standard deviation: ", sd(means), "\n")
#cat("sigma_xbar: ", sd(c(1,2,3,4,5,6))/sqrt(n), "\n")
#cat("Percentage error:", 100*(sd(means)-sd(c(1,2,3,4,5,6))/sqrt(n))/(sd(c(1,2,3,4,5,6))/sqrt(n)), "\n")
cat("Population sd 1:", sd(c(1,2,3,4,5,6))/sqrt(n), "\n")
cat("Population sd 2", sqrt(35/12 / n))
}
diceN(1000)
diceN(1000)
diceN(100)
sd(1:6)
sqrt(35/12)
diceN <- function(n) {
randDice = matrix(sample.int(6, size=n*n, replace=TRUE), nrow=n, ncol=n, byrow=TRUE)
means = apply(randDice, 1, mean)
hist(means, breaks=20, col="red", xlab="Sample Means")
#cat("Sample mean: ", mean(means), "\n")
#cat("Sample standard deviation: ", sd(means), "\n")
#cat("sigma_xbar: ", sd(c(1,2,3,4,5,6))/sqrt(n), "\n")
#cat("Percentage error:", 100*(sd(means)-sd(c(1,2,3,4,5,6))/sqrt(n))/(sd(c(1,2,3,4,5,6))/sqrt(n)), "\n")
#cat("Population sd 1:", sd(c(1,2,3,4,5,6))/sqrt(n), "\n")
#cat("Population sd 2", sqrt(35/12 / n))
percent.error  <- 100*(sd(means) - sqrt(35/12/n))
return(percent.error)
}
percent.diff <- rep(0,1000)
for(i in 1:1000){
percent.diff[i] <- diceN(100)
}
hist(percent.diff)
?sd
diceN <- function(n) {
randDice = matrix(sample.int(6, size=n*n, replace=TRUE), nrow=n, ncol=n, byrow=TRUE)
means = apply(randDice, 1, mean)
hist(means, breaks=20, col="red", xlab="Sample Means")
#cat("Sample mean: ", mean(means), "\n")
#cat("Sample standard deviation: ", sd(means), "\n")
#cat("sigma_xbar: ", sd(c(1,2,3,4,5,6))/sqrt(n), "\n")
#cat("Percentage error:", 100*(sd(means)-sd(c(1,2,3,4,5,6))/sqrt(n))/(sd(c(1,2,3,4,5,6))/sqrt(n)), "\n")
#cat("Population sd 1:", sd(c(1,2,3,4,5,6))/sqrt(n), "\n")
#cat("Population sd 2", sqrt(35/12 / n))
percent.error  <- 100*(sd(means) - sqrt(35/12/n))/sqrt(35/12/n)
return(percent.error)
}
percent.diff <- rep(0,1000)
for(i in 1:1000){
percent.diff[i] <- diceN(100)
}
hist(percent.diff)
58.17+125.21+60.53+104.29+49.99+294.05+106.99+240.79+60.15+67.19+47.98
99 + 20*12
mean(c(14,15,13,21,15,15,1000,16,20,13))
median(c(14,15,13,21,15,15,1000,16,20,13))
?qbeta
mu = c(.5,.85)
phi = 2
x = c(0.2, 0.9)
qbeta(x, mu*phi, 1 - mu*phi)
mu*phi
1 - mu*phi
x
qbeta(x, mu*phi, \phi - mu*phi)
qbeta(x, mu*phi, phi - mu*phi)
??rbeta
p = c(0.4,0.2,0.8)
mu <- c(0.6, 0.6, 0.7)
phi <- 1
qbeta(p, mu*phi, (1-mu)*phi)
require(igraph)
g <- ring(10)
plot(g, layout=layout_with_kk, vertex.color="green")
plot(g, layout=layout_with_kk, vertex.color="green")
class(g)
2/sqrt(pi)
library(devtools)
library(roxygen2)
install.packages("RCurl")
library(RCurl)
setwd("/Users/jdwilson4/Dropbox/Research/Multilayer_Extraction")
setwd("./MultilayerExtraction")
devtools::install_github('jdwilson4/multilayer_extraction')
library(MultilayerExtraction)
?multilayer.extraction
data(AU_CS)
install.packages("repmis")
library(repmis)
?source_data
source_data("https://github.com/jdwilson4/multilayer_extraction/data/AU_CS.Rdata?raw=TRUE")
setwd("/Users/jdwilson4/Dropbox/Research/Multilayer_Extraction")
setwd("./MultilayerExtraction")
document()
document()
load("/Users/jdwilson4/Dropbox/Research/Multilayer_Extraction/MultilayerExtraction/AU_CS.rda")
setwd("/Users/jdwilson4/Dropbox/Research/Multilayer_Extraction")
setwd("./MultilayerExtraction")
document()
devtools::install_github('jdwilson4/multilayer_extraction')
library(MultilayerExtraction)
rm(list = ls())
data("AU_CS")
n <- dim(Adjacency[[1]])[1]
m <- length(Adjacency)
relationship.names <- names(Adjacency)
print(relationship.names)
load("/Users/jdwilson4/Dropbox/Research/Multilayer_Extraction/Github Code/data/AU_CS.Rdata")
load("/Users/jdwilson4/Dropbox/Research/Multilayer_Extraction/MultilayerExtraction/data/AU_CS.rda")
data(arXiv)
length(Adjacency)
data(AU_CS)
length(Adjacency)
data("EU_Airport")
length(Adjacency)
setwd("/Users/jdwilson4/Dropbox/Research/Multilayer_Extraction")
setwd("./MultilayerExtraction")
document()
devtools::install_github('jdwilson4/multilayer_extraction')
library(MultilayerExtraction)
rm(list = ls())
data("AU_CS")
length(Adjacency)
data(AU_CS)
length(Adjacency)
?use_data
devtools::install_github('jdwilson4/multilayer_extraction')
library(MultilayerExtraction)
data("AU_CS")
devtools::install_github('jdwilson4/multilayer_extraction')
library(MultilayerExtraction)
?AU_CS
load("data-raw/AU_CS.rda")
AU_CS <- Adjacency
devtools::use_data(AU_CS, overwrite = TRUE)
load("data-raw/arXiv.rda")
arXiv <- Adjacency
names(arXiv) <- layer.labels
devtools::use_data(arXiv, overwrite = TRUE)
load("data-raw/EU_Airport.rda")
EU_Airport <- Adjacency
names(EU_Airport) <- layer.labels
devtools::use_data(EU_Airport, overwrite = TRUE)
devtools::install_github('jdwilson4/multilayer_extraction')
devtools::install_github('jdwilson4/multilayer_extraction')
library(MultilayerExtraction)
data("AU_CS")
rm(list = ls())
data("AU_CS")
n <- dim(Adjacency[[1]])[1]
n <- dim(AU_CS[[1]])[1]
m <- length(AU_CS)
n
relationship.names <- names(AU_CS)
print(relationship.names)
devtools::install_github('jdwilson4/multilayer_extraction')
library(MultilayerExtraction)
data("arXiv")
data("AU_CS")
n <- dim(AU_CS[[1]])[1]
m <- length(AU_CS)
relationship.names <- names(AU_CS)
print(relationship.names)
image(AU_CS$coauthor)
image(AU_CS$facebook)
image(AU_CS$leisure)
Mean.Degree <- sapply(AU_CS, mean(rowSums))
Mean.Degree <- sapply(AU_CS, mean)
Mean.Degree <- sapply(AU_CS, mean*n)
n <- dim(AU_CS[[1]])[1]
Mean.Degree <- sapply(AU_CS, mean)*n
Mean.Degre
Mean.Degree
?multilayer.extraction
community.object <- multilayer.extraction(adjacency, seed = 123, min.score = 0, prop.sample = 1.00)
library(MultilayerExtraction)
devtools::install_github('jdwilson4/multilayer_extraction')
library(MultilayerExtraction)
devtools::install_github('jdwilson4/multilayer_extraction', force = TRUE)
library(MultilayerExtraction)
